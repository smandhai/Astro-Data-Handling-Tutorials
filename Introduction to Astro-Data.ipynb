{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbfb25a",
   "metadata": {},
   "source": [
    "# Introduction To Astro-Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc9cd3",
   "metadata": {},
   "source": [
    "In this notebook, I will cover the key methods required to get started with processing astrophysical datasets.\n",
    "\n",
    "We will cover:\n",
    "- Downloading datasets directly from the source using scripts\n",
    "- Dealing with different file formats\n",
    "- Loading in data effectively\n",
    "- Processing and manipulating data\n",
    "- Visulation\n",
    "- Storing Data\n",
    "\n",
    "## Case Study\n",
    "Gamma-Ray Bursts (GRBs) are among the highest energy events that arise within the Universe. They can outshine galaxies and can be seen at extremely high redshift. There are two primary classes of GRBs, short and long. These are characterised by the time taken for 5\\% - 95\\% of the flux to be detected (T90). Short GRBs have T90s typically <2 s, whilst for Long GRBs, the T90s are >2 s.  \n",
    "\n",
    "In this example, we will:\n",
    "- Download some GRB light curves\n",
    "- Visualise\n",
    "- Download and visualise location skymaps corresponding to the GRB\n",
    "- Identify the nearest galaxies\n",
    "- Circle regions of interest\n",
    "- Output a list of nearby galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90624d",
   "metadata": {},
   "source": [
    "## Downloading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2dd599fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ TERR 1 2\n",
      "! WTSLEW\n",
      "69.9743357492305\t0.306271252653971\t-0.627777791207436\t6.78875744702424e-08\t6.70027921276708e-09\t-6.66919272114592e-09\n",
      "70.885004784499\t0.487963190743571\t-0.604397782614527\t5.35267896547529e-08\t5.30773676222583e-09\t-5.30095445235233e-09\n",
      "71.8514489319465\t0.526729085295955\t-0.478480956703848\t6.07191123146501e-08\t6.02363112725272e-09\t-6.01461100192462e-09\n",
      "73.0492055230983\t0.617135440845203\t-0.671027505855889\t4.77109292899582e-08\t4.74432085142327e-09\t-4.7325332779053e-09\n",
      "74.1514555612188\t0.61080238919007\t-0.485114597275341\t5.65116292099323e-08\t5.61720752794371e-09\t-5.61992247880915e-09\n",
      "75.3647542483539\t0.619742719541634\t-0.602496297944953\t4.95491660514466e-08\t4.99546134007864e-09\t-6.11798938573008e-09\n",
      "76.6296523856011\t0.600222599719899\t-0.64515541770561\t4.71570807981703e-08\t4.83860936275126e-09\t-7.69536479830198e-09\n",
      "78.0657704342574\t0.689686490181089\t-0.835895448936412\t5.15278550750758e-08\t4.43569334116616e-09\t-5.81480706277939e-09\n",
      "NO NO NO NO NO NO\n",
      "! WT\n",
      "83.302\t0.367\t-\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Conveniently, python has native packages that can help us download data directly from HTTP sources! \n",
    "The [urlretrieve] package is what we will be using to download the dataset that we are interested in.\"\"\"\n",
    "import urllib.request as req\n",
    "import shutil #for shell operations\n",
    "\"Download the data for GRB 211211A - Full list can be found here: https://www.swift.ac.uk/xrt_products/index.php?year=2022\"\n",
    "\"Click on the curve option for the GRB of interest and click the download button. You can either save the file directly or copy the url.\"\n",
    "\n",
    "\"Direct Download\"\n",
    "url  = \"https://www.swift.ac.uk/scripts/viewData.php?file=https://www.swift.ac.uk/user_objects/tprods/tmp_jsplot_GBz4DH.qdp\"\n",
    "text_file = 'GRB211211A_xrt.txt'\n",
    "try:\n",
    "    data = req.urlretrieve(url,text_file) #Legacy method\n",
    "except:\n",
    "    print(\"Legacy method has failed\")\n",
    "    \"Modern approach to downloading datasets\"\n",
    "    with req.urlopen(url) as response, open(text_file,'wb') as of:\n",
    "        shutil.copyfileobj(response,of)\n",
    "\"Let's preview the file\"\n",
    "f=open(text_file,'r') #opens the file\n",
    "print(f.read(1000)) #reads the first 100 characters\n",
    "f.close() #closes the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ef86b",
   "metadata": {},
   "source": [
    "We can see here that the datafile contains multiple chunks of different datapoints within its tables. This can make loading the file in tricky. However, there are ways to get around this! We will explore these below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0a615",
   "metadata": {},
   "source": [
    "## Loading in dataset\n",
    "Now that we have downloaded our data, we need to load it in, so we can start processing it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48b8905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4b791",
   "metadata": {},
   "source": [
    "### Using Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "08ac174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table length before NaN removal: 290\n",
      "Table length after NaN removal: 286\n",
      "                 time          terr+         terr-             f  \\\n",
      "0    69.9743357492305       0.306271     -0.627778  6.788757e-08   \n",
      "1     70.885004784499       0.487963     -0.604398  5.352679e-08   \n",
      "2    71.8514489319465       0.526729     -0.478481  6.071911e-08   \n",
      "3    73.0492055230983       0.617135     -0.671028  4.771093e-08   \n",
      "4    74.1514555612188       0.610802     -0.485115  5.651163e-08   \n",
      "..                ...            ...           ...           ...   \n",
      "283         62273.158     573.024000   -525.174000  1.169543e-12   \n",
      "284         67663.355     200.306000   -368.858000  1.419150e-12   \n",
      "285         68100.424     289.764000   -236.763000  1.968949e-12   \n",
      "286         75209.989    4660.091000  -2061.256000  8.086668e-13   \n",
      "287        118513.353  167656.747000 -28024.017000  1.453584e-13   \n",
      "\n",
      "               f+            f-  \n",
      "0    6.700279e-09 -6.669193e-09  \n",
      "1    5.307737e-09 -5.300954e-09  \n",
      "2    6.023631e-09 -6.014611e-09  \n",
      "3    4.744321e-09 -4.732533e-09  \n",
      "4    5.617208e-09 -5.619922e-09  \n",
      "..            ...           ...  \n",
      "283  2.477203e-13 -2.477203e-13  \n",
      "284  3.731964e-13 -3.731964e-13  \n",
      "285  4.557898e-13 -4.557898e-13  \n",
      "286  1.459977e-13 -1.459977e-13  \n",
      "287  2.651030e-14 -2.651030e-14  \n",
      "\n",
      "[286 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\"The dataset has 4 headers but 6 columns\"\n",
    "#print('NO '*6)\n",
    "\"\"\"\n",
    "As our datafile has some oddities, we will be loading it in with settings that allow us to skip inconsistent lines and flag NaNs.\n",
    "Parameter Description:\n",
    "1. The first parameter loads in the file corresponding to a string.\n",
    "2. 'sep' indicates the separation between columns. In this case, each column is separated by a tab.\n",
    "3. I have skipped the first row as it gives a general header which has inconsistent rows with the rest of the data.\n",
    "4. Here I have defined my own header names. As I knew what to expect from the data, I was able to name the columns. By\n",
    "default the headers will be numbers 0-N.\n",
    "5. I'm telling pandas that my data shouldn't have headers following the processing. This prevents the first data row from being read\n",
    "in as the header.\n",
    "6. As the datafile has some section breaks indicated by \"!\", I have marked lines with these as comments.\n",
    "7. The data sets in the table are separated by \"NO NO NO NO NO NO\" (one for each column). Here I have defined these as NaN values.\n",
    "\"\"\"\n",
    "data_table = pd.read_table(text_file,sep='\\t',skiprows=[0],names=[\"time\",\"terr+\",\"terr-\",\"f\",\"f+\",\"f-\"],\n",
    "                           header=None,comment='!',na_values=[('NO '*6)[:-1]])\n",
    "data_table.head() #previews the data table\n",
    "\"Note: We could use the NaN values to act as indicators of how the data is segmented.\"\n",
    "\n",
    "\"For the sake of this tutorial, we will be removing any NaN values and treating the data as one dataset\"\n",
    "\n",
    "print(\"Table length before NaN removal: {}\".format(len(data_table)))\n",
    "data_table = data_table.dropna(0)\n",
    "print(\"Table length after NaN removal: {}\".format(len(data_table)))\n",
    "print(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad7338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc08606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
